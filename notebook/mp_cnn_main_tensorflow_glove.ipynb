{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:00:48.304120Z",
     "start_time": "2017-10-10T10:00:45.734527Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "dataset = pd.read_csv(\"../train.csv\",encoding = 'utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:00:48.310099Z",
     "start_time": "2017-10-10T10:00:48.306092Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "maxlen = 30\n",
    "dim = 300\n",
    "random_state =100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:00:48.506139Z",
     "start_time": "2017-10-10T10:00:48.312090Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dataset.question1 = dataset.question1.astype(str)\n",
    "dataset.question2 = dataset.question2.astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:00:50.039736Z",
     "start_time": "2017-10-10T10:00:48.508142Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import nltk\n",
    "import re\n",
    "from nltk import word_tokenize\n",
    "def text_to_wordlist(text, remove_stopwords=False, stem_words=False):\n",
    "\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:00:50.277547Z",
     "start_time": "2017-10-10T10:00:50.040735Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "dataset['question1_n'] = dataset.question1.apply(lambda x :text_to_wordlist(x))\n",
    "dataset['question2_n'] = dataset.question2.apply(lambda x :text_to_wordlist(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:00:50.299441Z",
     "start_time": "2017-10-10T10:00:50.278440Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>qid1</th>\n",
       "      <th>qid2</th>\n",
       "      <th>question1</th>\n",
       "      <th>question2</th>\n",
       "      <th>is_duplicate</th>\n",
       "      <th>question1_n</th>\n",
       "      <th>question2_n</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>0</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "      <td>What is the step by step guide to invest in sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "      <td>0</td>\n",
       "      <td>What is the story of Kohinoor (Koh-i-Noor) Dia...</td>\n",
       "      <td>What would happen if the Indian government sto...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "      <td>0</td>\n",
       "      <td>How can I increase the speed of my internet co...</td>\n",
       "      <td>How can Internet speed be increased by hacking...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>8</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "      <td>0</td>\n",
       "      <td>Why am I mentally very lonely? How can I solve...</td>\n",
       "      <td>Find the remainder when [math]23^{24}[/math] i...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>10</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "      <td>0</td>\n",
       "      <td>Which one dissolve in water quikly sugar, salt...</td>\n",
       "      <td>Which fish would survive in salt water?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  qid1  qid2                                          question1  \\\n",
       "0   0     1     2  What is the step by step guide to invest in sh...   \n",
       "1   1     3     4  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2   2     5     6  How can I increase the speed of my internet co...   \n",
       "3   3     7     8  Why am I mentally very lonely? How can I solve...   \n",
       "4   4     9    10  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                           question2  is_duplicate  \\\n",
       "0  What is the step by step guide to invest in sh...             0   \n",
       "1  What would happen if the Indian government sto...             0   \n",
       "2  How can Internet speed be increased by hacking...             0   \n",
       "3  Find the remainder when [math]23^{24}[/math] i...             0   \n",
       "4            Which fish would survive in salt water?             0   \n",
       "\n",
       "                                         question1_n  \\\n",
       "0  What is the step by step guide to invest in sh...   \n",
       "1  What is the story of Kohinoor (Koh-i-Noor) Dia...   \n",
       "2  How can I increase the speed of my internet co...   \n",
       "3  Why am I mentally very lonely? How can I solve...   \n",
       "4  Which one dissolve in water quikly sugar, salt...   \n",
       "\n",
       "                                         question2_n  \n",
       "0  What is the step by step guide to invest in sh...  \n",
       "1  What would happen if the Indian government sto...  \n",
       "2  How can Internet speed be increased by hacking...  \n",
       "3  Find the remainder when [math]23^{24}[/math] i...  \n",
       "4            Which fish would survive in salt water?  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:01:05.698583Z",
     "start_time": "2017-10-10T10:00:50.301452Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:01:05.703528Z",
     "start_time": "2017-10-10T10:01:05.700527Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:01:16.149965Z",
     "start_time": "2017-10-10T10:01:05.705528Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "tokenizer.fit_on_texts(dataset.question1_n.tolist() + dataset.question2_n.tolist())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:01:24.609945Z",
     "start_time": "2017-10-10T10:01:16.150956Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset['question1_seq']= tokenizer.texts_to_sequences(dataset.question1_n)\n",
    "dataset['question2_seq']= tokenizer.texts_to_sequences(dataset.question2_n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:01:25.105894Z",
     "start_time": "2017-10-10T10:01:24.610886Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_df, test_df = train_test_split(dataset,test_size=0.2, random_state= random_state)\n",
    "test_df, val_df = train_test_split(test_df,test_size=0.5, random_state= random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:01:25.256902Z",
     "start_time": "2017-10-10T10:01:25.106807Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Anaconda3\\lib\\site-packages\\gensim\\utils.py:860: UserWarning: detected Windows; aliasing chunkize to chunkize_serial\n",
      "  warnings.warn(\"detected Windows; aliasing chunkize to chunkize_serial\")\n"
     ]
    }
   ],
   "source": [
    "from gensim.models import KeyedVectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:01:25.261894Z",
     "start_time": "2017-10-10T10:01:25.257866Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_word = len(tokenizer.word_index)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:01:25.269888Z",
     "start_time": "2017-10-10T10:01:25.263867Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def loadGloveModel(gloveFile):\n",
    "    f = open(gloveFile,'r',encoding='utf-8')\n",
    "    model = {}\n",
    "    for line in f:\n",
    "        splitLine = line.split()\n",
    "        word = splitLine[0]\n",
    "        if word not in tokenizer.word_index:\n",
    "            continue\n",
    "        try:\n",
    "            embedding = [float(val) for val in splitLine[1:]]\n",
    "            model[word] = embedding\n",
    "        except:\n",
    "            pass\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:02:20.083782Z",
     "start_time": "2017-10-10T10:01:25.271867Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "glove_dict = loadGloveModel(\"../glove.840B.300d.txt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:02:20.658004Z",
     "start_time": "2017-10-10T10:02:20.084750Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Null word embeddings: 29276\n"
     ]
    }
   ],
   "source": [
    "embedding_matrix = np.zeros((num_word+1,dim ),dtype='float32')\n",
    "for word, i in tokenizer.word_index.items():\n",
    "    if word in glove_dict:\n",
    "        embedding_matrix[i] = glove_dict[word]\n",
    "print('Null word embeddings: %d' % np.sum(np.sum(embedding_matrix, axis=1) == 0))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:02:20.667043Z",
     "start_time": "2017-10-10T10:02:20.659036Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(95597, 300)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "embedding_matrix.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:02:21.803174Z",
     "start_time": "2017-10-10T10:02:20.669009Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import time\n",
    "import numpy as np\n",
    "import tensorflow.contrib.layers as layers\n",
    "\n",
    "min_temp_result = None\n",
    "\n",
    "def create_filter_blockA_weight(n_grams, w_dim, num_kernel):\n",
    "    weight = tf.Variable(tf.random_normal((n_grams, w_dim, num_kernel),stddev=0.1),name='blockA_W')\n",
    "    # weight = tf.Variable(tf.ones((n_grams, w_dim, num_kernel)))\n",
    "    bias = tf.Variable(tf.zeros(num_kernel),name='blockA_b')\n",
    "    return weight , bias\n",
    "\n",
    "def create_filter_blockB_weight(n_grams, w_dim, num_kernel):\n",
    "    weight = tf.Variable(tf.random_normal((n_grams, 1, 1, num_kernel), stddev=0.1),name='blockB_W')\n",
    "    # weight = tf.Variable(tf.ones((n_grams, 1, 1, num_kernel)))\n",
    "    bias = tf.Variable(tf.zeros(num_kernel),name='blockB_b')\n",
    "    return weight,bias\n",
    "\n",
    "\n",
    "def create_horizontal_conv(input,sequence_length, kernel_weight, type, min_mask):\n",
    "    '''\n",
    "\n",
    "    :param input:  N x L x w_dim\n",
    "    :param kernel_weight: [ window_size, 1, 1, number_kernel]\n",
    "    :param type:\n",
    "    :return: N x w_dim x num_kernel\n",
    "    '''\n",
    "\n",
    "\n",
    "    with tf.name_scope(\"horizontal_conv\"):\n",
    "        i0 = tf.constant(0)\n",
    "        num_kernel = int(kernel_weight[0].get_shape()[-1])\n",
    "        # result = tf.zeros([0,w_dim, num_kernel])\n",
    "\n",
    "        input = tf.expand_dims(input, 3)\n",
    "        output = tf.nn.conv2d(input, kernel_weight[0], [1, 1, 1, 1], 'SAME') + kernel_weight[1]  # N, height, width, out_kernel\n",
    "        output = tf.nn.relu(output)\n",
    "        if type == 'min':\n",
    "            output = min_pool_operation2d(output,min_mask )\n",
    "        elif type == 'mean':\n",
    "            output = mean_pool_operation2d(output,sequence_length)\n",
    "        elif type == 'max':\n",
    "            output = tf.reduce_max(output, axis=1)  # N, out_kernel\n",
    "        else:\n",
    "            raise Exception(\"no such type\")\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "        # result = tf.while_loop(_cond, _run_conv, loop_vars=[i0, result],shape_invariants=[i0.get_shape(), tf.TensorShape([None, w_dim, num_kernel])])\n",
    "        # return result[1]\n",
    "\n",
    "\n",
    "\n",
    "def create_vertical_conv(input, sequence_length,  kernel_weight, type, min_mask):\n",
    "    '''\n",
    "\n",
    "    :param input:\n",
    "    :param sequence_length:\n",
    "    :param kernel_weight:\n",
    "    :param type:\n",
    "    :return:  N, num_kernel\n",
    "    '''\n",
    "    # num_kernel = 3\n",
    "    # filter = tf.Variable(tf.ones((2, 4, num_kernel)))\n",
    "    with tf.name_scope(\"vertical_conv\"):\n",
    "        num_kernel = int(kernel_weight[0].get_shape()[2])\n",
    "        output = tf.nn.conv1d(input, kernel_weight[0], 1,  'SAME') + kernel_weight[1]  # None, out_width  , out_kernel\n",
    "        output = tf.nn.relu(output)\n",
    "\n",
    "        if type == 'min':\n",
    "            output = min_pool_operation(output,min_mask )\n",
    "        elif type == 'mean':\n",
    "            output = mean_pool_operation(output,sequence_length)\n",
    "        elif type == 'max':\n",
    "            output = tf.reduce_max(output, axis=1)  # N, out_kernel\n",
    "        else:\n",
    "            raise Exception(\"no such type\")\n",
    "        return output\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def create_direct_pool(input,num_kernel, type):\n",
    "    '''\n",
    "\n",
    "    :param input:\n",
    "    :param num_kernel:\n",
    "    :param type:\n",
    "    :return: N, num_kernel\n",
    "    '''\n",
    "    if type == 'min':\n",
    "        output = tf.tile(tf.expand_dims(tf.reduce_min(tf.reduce_min(input, axis=1), axis=1),axis=1),[1,num_kernel])\n",
    "    elif type == 'mean':\n",
    "        output = tf.tile(tf.expand_dims(tf.reduce_mean(tf.reduce_mean(input, axis=1), axis=1),axis=1),[1,num_kernel])\n",
    "    elif type == 'max':\n",
    "        output = tf.tile(tf.expand_dims(tf.reduce_max(tf.reduce_max(input, axis=1), axis=1),axis=1),[1,num_kernel])\n",
    "    else:\n",
    "        raise Exception(\"no such type\")\n",
    "\n",
    "    return output\n",
    "\n",
    "def l2distance(input1,input2):\n",
    "    l2diff =tf.reduce_sum(tf.square(tf.subtract(input1, input2)),\n",
    "                                   axis=1)\n",
    "    l2diff = tf.clip_by_value(l2diff,0.1,1e7)\n",
    "    l2diff = tf.sqrt(l2diff)\n",
    "    return l2diff\n",
    "\n",
    "def l1distance(input1,input2):\n",
    "    l1diff = tf.square(tf.subtract(input1, input2))\n",
    "    l1diff = tf.sqrt(tf.clip_by_value(l2diff,0.1,1e7))\n",
    "    l1diff = tf.reduce_sum(l1diff, axis=1)\n",
    "    return l1diff\n",
    "\n",
    "def cosine_similarity(input1,input2):\n",
    "    n_input1 = tf.nn.l2_normalize(input1, dim=1,epsilon=1e-7)\n",
    "    n_input2 = tf.nn.l2_normalize(input2, dim=1,epsilon=1e-7)\n",
    "    cosine_sim = tf.reduce_sum(tf.multiply(n_input1, n_input2), axis=1)\n",
    "    return cosine_sim\n",
    "\n",
    "\n",
    "def pairwise_distance1(input1, input2):\n",
    "    '''\n",
    "    :param input1: N, num_kernel, 1\n",
    "    :param input2: N, num_kernel, 1\n",
    "    :return:\n",
    "    '''\n",
    "    with tf.name_scope(\"pairwise_distance1\"):\n",
    "        # return tf.stack([cosine_similarity(input1,input2),l2distance(input1,input2)],axis=1)\n",
    "        return tf.concat([cosine_similarity(input1,input2),l2distance(input1,input2)],axis=1)\n",
    "\n",
    "\n",
    "def pairwise_distance2(input1, input2):\n",
    "    '''\n",
    "    :param input1: N, num_kernel, 1\n",
    "    :param input2: N, num_kernel, 1\n",
    "    :return:\n",
    "    '''\n",
    "\n",
    "    with tf.name_scope(\"pairwise_distance2\"):\n",
    "        # return tf.stack([cosine_similarity(input1,input2),l2distance(input1,input2)],axis=1)\n",
    "        return tf.concat([cosine_similarity(input1,input2),l2distance(input1,input2)],axis=1)\n",
    "\n",
    "\n",
    "def get_init_min_mask_value(input_sequence):\n",
    "    value = np.zeros(shape=(input_sequence.shape[0],maxlen))\n",
    "    for i, l in enumerate(input_sequence):\n",
    "        value[i, l:] = 1e7\n",
    "    return value\n",
    "\n",
    "def min_pool_operation(tf_var, min_mask):\n",
    "    '''\n",
    "\n",
    "    :param tf_var:\n",
    "    :param min_mark:\n",
    "    :return:\n",
    "    '''\n",
    "    global min_temp_result\n",
    "    min_mask = tf.expand_dims(min_mask,axis=2)\n",
    "    # min_mask = tf.reshape(min_mask, (None,min_mask_mask.shape[0], tf_var.shape[2]))\n",
    "    temp = tf.add(tf_var, tf.cast(min_mask,tf.float32))\n",
    "    min_temp_result = temp\n",
    "    return tf.reduce_min(temp, axis=1) \n",
    "\n",
    "\n",
    "def mean_pool_operation(tf_var, input_sequence):\n",
    "    '''\n",
    "\n",
    "    :param tf_var:\n",
    "    :param min_mark:\n",
    "    :return:\n",
    "    '''\n",
    "    input_sequence = tf.reshape(input_sequence,[-1,1])\n",
    "    temp = tf.divide(tf.reduce_sum(tf_var,axis=1), tf.add(tf.cast(input_sequence,tf.float32),1e-7))\n",
    "    return temp\n",
    "\n",
    "\n",
    "def min_pool_operation2d(tf_var, min_mask):\n",
    "    '''\n",
    "\n",
    "    :param tf_var:\n",
    "    :param min_mark:\n",
    "    :return:\n",
    "    '''\n",
    "    min_mask = tf.expand_dims(tf.expand_dims(min_mask,2),3)\n",
    "    min_mask = tf.tile(min_mask,[1,1,int(tf_var.shape[2]),int(tf_var.shape[3])])\n",
    "    # min_mask = tf.reshape(min_mask, (None,min_mask_mask.shape[0], tf_var.shape[2]))\n",
    "    temp = tf.add(tf_var, tf.cast(min_mask,tf.float32))\n",
    "    return tf.reduce_min(temp, axis=1)\n",
    "\n",
    "\n",
    "def mean_pool_operation2d(tf_var, input_sequence):\n",
    "    '''\n",
    "\n",
    "    :param tf_var:\n",
    "    :param min_mark:\n",
    "    :return:\n",
    "    '''\n",
    "    input_sequence = tf.reshape(input_sequence,[-1,1,1])\n",
    "    input_sequence = tf.tile(input_sequence,[1,int(tf_var.shape[2]),int(tf_var.shape[3])])\n",
    "    temp = tf.divide(tf.reduce_sum(tf_var,axis=1), tf.add(tf.cast(input_sequence,tf.float32), 1e-7))\n",
    "    return temp\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "class MPCNN:\n",
    "    \n",
    "    \n",
    "    def __init__(self, maxlen, dim, embedding_weight):\n",
    "        \n",
    "        \n",
    "        self.input = tf.placeholder(tf.int32,(None,maxlen),name='input1')\n",
    "        self.input2 = tf.placeholder(tf.int32,(None,maxlen),name='input2')\n",
    "#         self.input = tf.placeholder(tf.float32,(None,maxlen, dim),name='input1')\n",
    "#         self.input2 = tf.placeholder(tf.float32,(None,maxlen, dim),name='input2')\n",
    "        self.seq_length1 = tf.placeholder(tf.int32,(None),name='seq_len_1')\n",
    "        self.seq_length2 = tf.placeholder(tf.int32,(None),name='seq_len_2')\n",
    "        self.min_mask1 = tf.placeholder(tf.int32, (None, maxlen),name='min_mask1')\n",
    "        self.min_mask2 = tf.placeholder(tf.int32, (None, maxlen),name='min_mask2')\n",
    "        self.num_kernel_a = 32\n",
    "        self.num_kernel_b = 32\n",
    "        self.embedding_weight = tf.Variable(embedding_weight, name=\"E_W\")\n",
    "        self.y = tf.placeholder(tf.int32, shape=(None,2),name='ans')\n",
    "        \n",
    "        \n",
    "        \n",
    "        input = tf.nn.embedding_lookup(self.embedding_weight, self.input)\n",
    "        input2 = tf.nn.embedding_lookup(self.embedding_weight, self.input2)\n",
    "#         input = self.input\n",
    "#         input2 = self.input2\n",
    "        \n",
    "        num_kernel_a = self.num_kernel_a\n",
    "        num_kernel_b = self.num_kernel_b\n",
    "        seq_length1 = self.seq_length1\n",
    "        seq_length2 = self.seq_length2\n",
    "        min_mask1 = self.min_mask1\n",
    "        min_mask2 = self.min_mask2\n",
    "        y = self.y\n",
    "        \n",
    "        w_dim = dim\n",
    "        \n",
    "        n_grams_types = list(range(1,4)) + [-1]\n",
    "        blockA_type= ['max','mean']\n",
    "        blockA_weights = {}\n",
    "        self.blockA_weights = blockA_weights\n",
    "        regularizers = []\n",
    "        for n_g in n_grams_types:\n",
    "            for type in blockA_type:\n",
    "                if n_g  == - 1:\n",
    "                    continue\n",
    "                t_w = create_filter_blockA_weight(n_g,w_dim, num_kernel_a)\n",
    "                regularizers.append(tf.nn.l2_loss(t_w[0]))\n",
    "                blockA_weights[(n_g,type)] = t_w\n",
    "\n",
    "        \n",
    "        blockA_convs = [{},{}]\n",
    "        self.blockA_convs = blockA_convs\n",
    "        for n_g in n_grams_types :\n",
    "            for type in blockA_type:\n",
    "                if n_g == -1 :\n",
    "                    blockA_convs[0][(n_g,type)] = create_direct_pool(input,num_kernel_a,type)\n",
    "                    blockA_convs[1][(n_g,type)] = create_direct_pool(input2,num_kernel_a,type)\n",
    "                else:\n",
    "                    t_w = blockA_weights[(n_g,type)]\n",
    "                    blockA_convs[0][(n_g,type)] = create_vertical_conv(input,seq_length1,t_w,type, min_mask1)\n",
    "                    blockA_convs[1][(n_g,type)] = create_vertical_conv(input2,seq_length2,t_w,type, min_mask2)\n",
    "\n",
    "\n",
    "        #---------- block B ------------------\n",
    "        blockB_type= ['max','mean']\n",
    "        blockB_weights = {}\n",
    "        self.blockA_weights = blockA_weights\n",
    "        for n_g in n_grams_types:\n",
    "            for type in blockB_type:\n",
    "                if n_g  == - 1:\n",
    "                    continue\n",
    "                t_w = create_filter_blockB_weight(n_g,w_dim, num_kernel_b)\n",
    "                regularizers.append(tf.nn.l2_loss(t_w[0]))\n",
    "                blockB_weights[(n_g,type)] = t_w\n",
    "\n",
    "\n",
    "        blockB_convs = [{},{}]\n",
    "        self.blockB_convs = blockB_convs\n",
    "        for n_g in n_grams_types :\n",
    "            for type in blockB_type:\n",
    "                if n_g == -1 :\n",
    "                    continue\n",
    "                else:\n",
    "                    t_w = blockB_weights[(n_g,type)]\n",
    "                    blockB_convs[0][(n_g,type)] = create_horizontal_conv(input,seq_length1,t_w,type,min_mask1)\n",
    "                    blockB_convs[1][(n_g,type)] = create_horizontal_conv(input2,seq_length2,t_w,type,min_mask2)\n",
    "\n",
    "\n",
    "                    \n",
    "        outputs = []\n",
    "        #------------vertical-----comparison -------------\n",
    "\n",
    "        with tf.name_scope(\"vertical_comparison\"):\n",
    "            vertical_gp1 = []\n",
    "            vertical_gp2 = []\n",
    "            for type in blockA_type:\n",
    "                for n_g1 in n_grams_types:\n",
    "                    o1 = blockA_convs[0][(n_g1, type)]\n",
    "                    for n_g2 in n_grams_types:\n",
    "                        o2 = blockA_convs[1][(n_g2, type)]\n",
    "                        print(n_g1,n_g2, type)\n",
    "                        vertical_gp1.append(o1)\n",
    "                        vertical_gp2.append(o2)\n",
    "\n",
    "            vertical_gp1 = tf.stack(vertical_gp1,axis=2)\n",
    "            vertical_gp2 = tf.stack(vertical_gp2,axis=2)\n",
    "            self.temp_gp1 = vertical_gp1\n",
    "            self.temp_gp2 = vertical_gp2\n",
    "            o = pairwise_distance1(vertical_gp1, vertical_gp2)\n",
    "            outputs.append(o)\n",
    "\n",
    "\n",
    "            vertical_gp1 = []\n",
    "            vertical_gp2 = []\n",
    "            for n_g in n_grams_types:\n",
    "                if n_g == -1:\n",
    "                    continue\n",
    "                for type in blockB_type:\n",
    "                    vertical_gp1.append(blockB_convs[0][(n_g, type)])\n",
    "                    vertical_gp2.append(blockB_convs[1][(n_g, type)])\n",
    "          \n",
    "            vertical_gp1 = tf.concat(vertical_gp1,axis=2)\n",
    "            vertical_gp2 = tf.concat(vertical_gp2,axis=2)\n",
    "            self.temp_gp1 = vertical_gp1\n",
    "            self.temp_gp2 = vertical_gp2\n",
    "            o = pairwise_distance1(vertical_gp1, vertical_gp2)\n",
    "            outputs.append(o)\n",
    " \n",
    "\n",
    "        #-----------horizontal----comparison -------------------\n",
    "        with tf.name_scope(\"horizontal_comparison\"):\n",
    "            gp1 =[]\n",
    "            gp2 =[]\n",
    "            for type in blockA_type:\n",
    "                # r1 = []\n",
    "                # r2 = []\n",
    "                for n_g1 in n_grams_types:\n",
    "                    gp1.append(blockA_convs[0][(n_g1, type)]) # N, num_kernel\n",
    "                    gp2.append(blockA_convs[1][(n_g1, type)]) # N, num_kernel\n",
    "         \n",
    "            gp1 = tf.reshape(tf.concat(gp1,axis=1),(-1,len(n_grams_types),num_kernel_a * len(blockA_type)))\n",
    "            gp2 = tf.reshape(tf.concat(gp2,axis=1),(-1, len(n_grams_types), num_kernel_a  * len(blockA_type)))\n",
    "            o = pairwise_distance2(gp1, gp2)\n",
    "            outputs.append(o) \n",
    "\n",
    "        self.outputs = outputs\n",
    "        concat_output = tf.concat(outputs,axis=1)\n",
    "        self.concat_output = concat_output\n",
    "\n",
    "#         fc_ol = layers.fully_connected(concat_output, 64)\n",
    "        \n",
    "       \n",
    "        \n",
    "        def create_fc_layer(num_node, prev_input):\n",
    "            weight = tf.Variable(tf.truncated_normal([int(prev_input.shape[1]), num_node],stddev=0.1),name='fc_W')\n",
    "            regularizers.append(tf.nn.l2_loss(weight))\n",
    "            fc_biases_1 = tf.Variable(tf.zeros([num_node]),name='fc_b')\n",
    "            output = tf.nn.elu(tf.matmul(prev_input,weight) + fc_biases_1)\n",
    "            return output\n",
    "       \n",
    "        prob = tf.placeholder_with_default(1.0, shape=())\n",
    "        self.prob = prob\n",
    "        \n",
    "        concat_output = tf.nn.dropout(concat_output, prob)\n",
    "        fc_output = create_fc_layer(64, concat_output)\n",
    "        concat_output = tf.nn.dropout(fc_output, prob)\n",
    "        output = create_fc_layer(2, fc_output)\n",
    "        \n",
    "        \n",
    "        self.output = output\n",
    "\n",
    "        self.pred = tf.nn.softmax(output,dim=1)\n",
    "\n",
    "        self.total_loss = tf.nn.softmax_cross_entropy_with_logits(labels=y, logits=output)\n",
    "        self.loss = tf.reduce_mean(self.total_loss)\n",
    "        total_l2_loss = tf.zeros(1)\n",
    "        for r in regularizers:\n",
    "            total_l2_loss += r\n",
    "        self.loss += 1e-7 * total_l2_loss\n",
    "            \n",
    "        self.optimizer = tf.train.AdamOptimizer().minimize(self.loss)\n",
    "        \n",
    "        self.sess = tf.Session()\n",
    "        self.init = tf.global_variables_initializer()\n",
    "        self.sess.run(self.init)\n",
    "        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T09:46:40.281881Z",
     "start_time": "2017-10-10T09:46:40.272884Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:02:21.892184Z",
     "start_time": "2017-10-10T10:02:21.804175Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:05:05.621849Z",
     "start_time": "2017-10-10T10:04:58.717536Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 1 max\n",
      "1 2 max\n",
      "1 3 max\n",
      "1 -1 max\n",
      "2 1 max\n",
      "2 2 max\n",
      "2 3 max\n",
      "2 -1 max\n",
      "3 1 max\n",
      "3 2 max\n",
      "3 3 max\n",
      "3 -1 max\n",
      "-1 1 max\n",
      "-1 2 max\n",
      "-1 3 max\n",
      "-1 -1 max\n",
      "1 1 mean\n",
      "1 2 mean\n",
      "1 3 mean\n",
      "1 -1 mean\n",
      "2 1 mean\n",
      "2 2 mean\n",
      "2 3 mean\n",
      "2 -1 mean\n",
      "3 1 mean\n",
      "3 2 mean\n",
      "3 3 mean\n",
      "3 -1 mean\n",
      "-1 1 mean\n",
      "-1 2 mean\n",
      "-1 3 mean\n",
      "-1 -1 mean\n"
     ]
    }
   ],
   "source": [
    "\n",
    "model = MPCNN(maxlen,dim,embedding_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-01T16:59:22.618312Z",
     "start_time": "2017-10-01T16:59:22.536303Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-01T16:22:54.819588Z",
     "start_time": "2017-10-01T16:22:54.717076Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:05:05.629736Z",
     "start_time": "2017-10-10T10:05:05.622751Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_feature_X(df,maxlen):\n",
    "    x1 = []\n",
    "    x2 = []\n",
    "    l1 = []\n",
    "    l2 = []\n",
    "    for q1, q2 in zip(df.question1_seq.values, df.question2_seq.values):\n",
    "        \n",
    "        x1.append(q1)\n",
    "        x2.append(q2)\n",
    "        \n",
    "#         x1.append([embedding_matrix[t] for t in q1])\n",
    "#         x2.append([embedding_matrix[t] for t in q2])\n",
    "        l1.append(len(q1))\n",
    "        l2.append(len(q2))\n",
    "    \n",
    "    return pad_sequences(x1,maxlen,padding='post'), pad_sequences(x2,maxlen,padding='post'), np.array(l1),np.array(l2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:09:46.117569Z",
     "start_time": "2017-10-10T10:09:45.949764Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "from ipywidgets import FloatProgress\n",
    "import time\n",
    "from IPython.display import display\n",
    "def evaluate (self, df , is_training, batch_size, sess, dropout_prob = 0.2):\n",
    "    X = get_feature_X(df,maxlen)\n",
    "    Y = pd.get_dummies(df.is_duplicate)\n",
    "    sess = self.sess\n",
    "    start_index = 0\n",
    "    final_loss = 0\n",
    "    current_total_trained =0  \n",
    "    p_bar = FloatProgress()\n",
    "    display(p_bar)\n",
    "    start_time = time.time()\n",
    "    while start_index < X[0].shape[0]:\n",
    "        temp_x1 = X[0][start_index:start_index+batch_size]\n",
    "        temp_x2 = X[1][start_index:start_index+batch_size]\n",
    "        temp_seq_len1 = X[2][start_index:start_index+batch_size]\n",
    "        temp_seq_len2 = X[3][start_index:start_index+batch_size]\n",
    "        test_y = Y[start_index:start_index+batch_size]\n",
    "\n",
    "        feed_dict = {\n",
    "            self.min_mask1: get_init_min_mask_value(temp_seq_len1),\n",
    "            self.min_mask2: get_init_min_mask_value(temp_seq_len2),\n",
    "            self.seq_length1: temp_seq_len1,\n",
    "            self.seq_length2: temp_seq_len2,\n",
    "            self.input: temp_x1,\n",
    "            self.input2: temp_x2,\n",
    "            self.y: test_y\n",
    "        }\n",
    "        \n",
    "        if is_training:\n",
    "            feed_dict[self.prob] = 1 - dropout_prob\n",
    "        \n",
    "        current_total_trained += temp_x1.shape[0]\n",
    "\n",
    "        if is_training:\n",
    "            # the exact output you're looking for:\n",
    "            _, c =  sess.run([self.optimizer, self.loss], feed_dict=feed_dict)\n",
    "            final_loss += c * temp_x1.shape[0]\n",
    "            #print(\"%s/%s training loss %s\"  % (start_index, X[0].shape[0], final_loss/current_total_trained))\n",
    "#             sys.stdout.write(\"\\r%s/%s training loss %s\"  % (start_index, X[0].shape[0], c))\n",
    "#             sys.stdout.flush()\n",
    "            duration = time.time() - start_time\n",
    "            speed = duration/current_total_trained\n",
    "            eta = (X[0].shape[0]-current_total_trained)*speed\n",
    "            p_bar.value = current_total_trained/X[0].shape[0]\n",
    "            p_bar.description = \"%s/%s, eta %s sec\"%(current_total_trained, X[0].shape[0], eta)\n",
    "        else:\n",
    "            c =  sess.run(self.loss, feed_dict=feed_dict)\n",
    "            final_loss += c * temp_x1.shape[0]\n",
    "        start_index += batch_size\n",
    "        \n",
    "    final_loss = final_loss/X[0].shape[0]\n",
    "    return final_loss\n",
    "\n",
    "def gradients (self, df , batch_size, sess):\n",
    "    X = get_feature_X(df,maxlen)\n",
    "    Y = pd.get_dummies(df.is_duplicate)\n",
    "    sess = self.sess\n",
    "    start_index = 0\n",
    "    final_loss = 0\n",
    "    current_total_trained =0  \n",
    "    p_bar = FloatProgress()\n",
    "    display(p_bar)\n",
    "    start_time = time.time()\n",
    "    while start_index < X[0].shape[0]:\n",
    "        temp_x1 = X[0][start_index:start_index+batch_size]\n",
    "        temp_x2 = X[1][start_index:start_index+batch_size]\n",
    "        temp_seq_len1 = X[2][start_index:start_index+batch_size]\n",
    "        temp_seq_len2 = X[3][start_index:start_index+batch_size]\n",
    "        test_y = Y[start_index:start_index+batch_size]\n",
    "\n",
    "        feed_dict = {\n",
    "            self.min_mask1: get_init_min_mask_value(temp_seq_len1),\n",
    "            self.min_mask2: get_init_min_mask_value(temp_seq_len2),\n",
    "            self.seq_length1: temp_seq_len1,\n",
    "            self.seq_length2: temp_seq_len2,\n",
    "            self.input: temp_x1,\n",
    "            self.input2: temp_x2,\n",
    "            self.y: test_y\n",
    "        }\n",
    "        \n",
    "      \n",
    "        current_total_trained += temp_x1.shape[0]\n",
    "        \n",
    "        var_grad = tf.gradients(self.loss, [self.output])[0]\n",
    " \n",
    "        # the exact output you're looking for:\n",
    "        g =  sess.run([var_grad, self.concat_output], feed_dict=feed_dict)\n",
    "        print(\"gradient %s\"  % (g))\n",
    "#             sys.stdout.write(\"\\r%s/%s training loss %s\"  % (start_index, X[0].shape[0], c))\n",
    "#             sys.stdout.flush()\n",
    "        duration = time.time() - start_time\n",
    "        speed = duration/current_total_trained\n",
    "        eta = (X[0].shape[0]-current_total_trained)*speed\n",
    "        p_bar.value = current_total_trained/X[0].shape[0]\n",
    "        p_bar.description = \"%s/%s, eta %s sec\"%(current_total_trained, X[0].shape[0], eta)\n",
    "\n",
    "        start_index += batch_size\n",
    "        break\n",
    "        \n",
    "    final_loss = final_loss/X[0].shape[0]\n",
    "    return final_loss\n",
    "\n",
    "\n",
    "def fit(self,train_df , val_df, epochs,dropout_prob=0.2, batch_size=64,  check_point_name=\"./default_cnn_model\"):\n",
    "\n",
    "    sess = self.sess\n",
    "    \n",
    "    saver = tf.train.Saver(tf.global_variables ())\n",
    "    best_epoch = 0\n",
    "    best_loss = 1e9\n",
    "    os.mkdir(check_point_name)\n",
    "#     saver.save(self.sess, check_point_name+'/model', global_step=0)\n",
    "    for i in range (epochs):\n",
    "        print(\"training epoch \",i)\n",
    "        train_loss = evaluate(self,train_df,True,batch_size, sess, dropout_prob=dropout_prob)\n",
    "        print(\"train loss:\",train_loss)\n",
    "        loss = evaluate(self,val_df,False,64,sess)\n",
    "        print(\"val loss:\", loss)\n",
    "        if loss < best_loss:\n",
    "            best_epoch = i\n",
    "            best_loss = loss\n",
    "            print(\"save best_epoch %s to %s\"%(best_epoch,check_point_name))\n",
    "            saver.save(self.sess, check_point_name+'/model', global_step=i)\n",
    "            \n",
    "    return best_loss\n",
    "            \n",
    "            \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:09:48.011793Z",
     "start_time": "2017-10-10T10:09:48.009764Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#gradients(model,train_df,32,model.sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:09:50.513911Z",
     "start_time": "2017-10-10T10:09:50.510880Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#model.sess.run(model.init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:09:50.819297Z",
     "start_time": "2017-10-10T10:09:50.816316Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#fit(model, train_df, val_df,epochs=2,dropout_prob=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T09:50:42.170063Z",
     "start_time": "2017-10-10T09:50:42.115537Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T10:09:52.974076Z",
     "start_time": "2017-10-10T10:09:52.971041Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# net(input1[0],input2[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T11:18:41.362552Z",
     "start_time": "2017-10-10T11:18:41.354529Z"
    },
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "def tunning_model(model):\n",
    "    dropouts = [0.1,0.2,0.3,0.4,0.5]\n",
    "    for d in dropouts:\n",
    "        print(\"train with dropout %s\"%(d))\n",
    "        model.sess.run(model.init)\n",
    "        best_loss = fit(model, train_df, val_df,epochs=5,dropout_prob=d, check_point_name=\"./mpcnn_model_%s\"%(d)) \n",
    "        with open('mpcnn_val_result.txt','a') as f:\n",
    "            f.write(str({'dropout':d,'score':best_loss})+\"\\n\")\n",
    "        \n",
    "        \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T19:09:08.961718Z",
     "start_time": "2017-10-10T11:18:42.219387Z"
    },
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train with dropout 0.1\n",
      "training epoch  0\n",
      "train loss: [ 0.4020807]\n",
      "val loss: [ 0.33648765]\n",
      "save best_epoch 0 to ./mpcnn_model_0.1\n",
      "training epoch  1\n",
      "train loss: [ 0.2753706]\n",
      "val loss: [ 0.32164857]\n",
      "save best_epoch 1 to ./mpcnn_model_0.1\n",
      "training epoch  2\n",
      "train loss: [ 0.1972707]\n",
      "val loss: [ 0.33117303]\n",
      "training epoch  3\n",
      "train loss: [ 0.14837924]\n",
      "val loss: [ 0.37190205]\n",
      "training epoch  4\n",
      "train loss: [ 0.1174761]\n",
      "val loss: [ 0.39617136]\n",
      "train with dropout 0.2\n",
      "training epoch  0\n",
      "train loss: [ 0.4107666]\n",
      "val loss: [ 0.34441653]\n",
      "save best_epoch 0 to ./mpcnn_model_0.2\n",
      "training epoch  1\n",
      "train loss: [ 0.2943381]\n",
      "val loss: [ 0.32050586]\n",
      "save best_epoch 1 to ./mpcnn_model_0.2\n",
      "training epoch  2\n",
      "train loss: [ 0.22708826]\n",
      "val loss: [ 0.31134441]\n",
      "save best_epoch 2 to ./mpcnn_model_0.2\n",
      "training epoch  3\n",
      "train loss: [ 0.17813384]\n",
      "val loss: [ 0.32781377]\n",
      "training epoch  4\n",
      "train loss: [ 0.14578272]\n",
      "val loss: [ 0.35247904]\n",
      "train with dropout 0.3\n",
      "training epoch  0\n",
      "train loss: [ 0.42007941]\n",
      "val loss: [ 0.34526026]\n",
      "save best_epoch 0 to ./mpcnn_model_0.3\n",
      "training epoch  1\n",
      "train loss: [ 0.31070246]\n",
      "val loss: [ 0.31490713]\n",
      "save best_epoch 1 to ./mpcnn_model_0.3\n",
      "training epoch  2\n",
      "train loss: [ 0.24837659]\n",
      "val loss: [ 0.31280896]\n",
      "save best_epoch 2 to ./mpcnn_model_0.3\n",
      "training epoch  3\n",
      "train loss: [ 0.2031508]\n",
      "val loss: [ 0.32991824]\n",
      "training epoch  4\n",
      "train loss: [ 0.16859915]\n",
      "val loss: [ 0.4001064]\n",
      "train with dropout 0.4\n",
      "training epoch  0\n",
      "train loss: [ 0.43041558]\n",
      "val loss: [ 0.35014799]\n",
      "save best_epoch 0 to ./mpcnn_model_0.4\n",
      "training epoch  1\n",
      "train loss: [ 0.32702899]\n",
      "val loss: [ 0.32191315]\n",
      "save best_epoch 1 to ./mpcnn_model_0.4\n",
      "training epoch  2\n",
      "train loss: [ 0.26998759]\n",
      "val loss: [ 0.31496465]\n",
      "save best_epoch 2 to ./mpcnn_model_0.4\n",
      "training epoch  3\n",
      "train loss: [ 0.22695582]\n",
      "val loss: [ 0.36532718]\n",
      "training epoch  4\n",
      "train loss: [ 0.18925858]\n",
      "val loss: [ 0.49196741]\n",
      "train with dropout 0.5\n",
      "training epoch  0\n",
      "train loss: [ 0.44802912]\n",
      "val loss: [ 0.36522847]\n",
      "save best_epoch 0 to ./mpcnn_model_0.5\n",
      "training epoch  1\n",
      "train loss: [ 0.34752958]\n",
      "val loss: [ 0.33604804]\n",
      "save best_epoch 1 to ./mpcnn_model_0.5\n",
      "training epoch  2\n",
      "train loss: [ 0.297434]\n",
      "val loss: [ 0.32734838]\n",
      "save best_epoch 2 to ./mpcnn_model_0.5\n",
      "training epoch  3\n",
      "train loss: [ 0.25662318]\n",
      "val loss: [ 0.38270849]\n",
      "training epoch  4\n",
      "train loss: [ 0.22303744]\n",
      "val loss: [ 0.47940472]\n"
     ]
    }
   ],
   "source": [
    "tunning_model(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-06T22:24:19.991058Z",
     "start_time": "2017-10-06T22:24:19.971551Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-04T21:14:31.692463Z",
     "start_time": "2017-10-04T21:14:31.687432Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T20:33:59.862215Z",
     "start_time": "2017-10-10T20:33:59.842201Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'dropout': 0.1, 'score': array([ 0.32164857], dtype=float32)}\n",
      "\n",
      "{'dropout': 0.2, 'score': array([ 0.31134441], dtype=float32)}\n",
      "\n",
      "{'dropout': 0.3, 'score': array([ 0.31280896], dtype=float32)}\n",
      "\n",
      "{'dropout': 0.4, 'score': array([ 0.31496465], dtype=float32)}\n",
      "\n",
      "{'dropout': 0.5, 'score': array([ 0.32734838], dtype=float32)}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "for line in open('./mpcnn_val_result.txt','r'):\n",
    "    print(line)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dropout 0.2 is best, as loss function = 0.31134441"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T19:42:00.755281Z",
     "start_time": "2017-10-10T19:42:00.057778Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T19:42:12.529669Z",
     "start_time": "2017-10-10T19:42:11.888164Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./mpcnn_model_0.2/model-2\n"
     ]
    }
   ],
   "source": [
    "saver.restore(model.sess,'./mpcnn_model_0.2/model-2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T19:42:14.108162Z",
     "start_time": "2017-10-10T19:42:14.104132Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<__main__.MPCNN at 0x13f577511d0>"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-06T22:18:01.472944Z",
     "start_time": "2017-10-06T22:18:01.469442Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T19:42:54.680516Z",
     "start_time": "2017-10-10T19:42:15.761496Z"
    },
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "test_loss = evaluate(model,test_df,False,64,model.sess)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2017-10-10T19:42:57.292982Z",
     "start_time": "2017-10-10T19:42:57.288953Z"
    },
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.3132183], dtype=float32)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
